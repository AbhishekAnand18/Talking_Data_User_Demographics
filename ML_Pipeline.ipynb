{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization,Input,PReLU\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1_1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=input_shape))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_1_2(input_dim,output_dim, learRate=0.0025):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_shape=(input_dim,), init='uniform'))\n",
    "    model.add(PReLU(init='zero'))\n",
    "    model.add(Dropout(0.82))\n",
    "    model.add(Dense(output_dim, init='uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "    opt = Adagrad(lr=learRate, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2_1(input_dim,output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.15, input_shape=(input_dim,)))\n",
    "    model.add(Dense(240, init='uniform'))\n",
    "    model.add(PReLU(init='zero'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(240, init='uniform'))\n",
    "    model.add(PReLU(init='zero', weights=None))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(260, init='uniform'))\n",
    "    model.add(PReLU(init='zero', weights=None))\n",
    "    model.add(Dropout(0.40))\n",
    "    model.add(Dense(output_dim, init='uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    opt = Adagrad(lr=0.008, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2_2(input_dim,output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.4, input_shape=(input_dim,)))\n",
    "    model.add(Dense(75))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(50, init='normal', activation='tanh'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Dense(output_dim, init='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('gender_age_train.csv')\n",
    "test_data=pd.read_csv('gender_age_test.csv')\n",
    "events = pd.read_csv('events.csv',  parse_dates=['timestamp'], index_col='event_id')\n",
    "phone_data=pd.read_csv('phone_brand_device_model.csv')\n",
    "# We Need to Drop Duplicate Devices and set Device_id as index like we did for Data while importing\n",
    "phone_data = phone_data.drop_duplicates('device_id',keep='first').set_index('device_id')\n",
    "app_events = pd.read_csv('app_events.csv', usecols=['event_id','app_id','is_active'], dtype={'is_active':bool})\n",
    "app_labels = pd.read_csv('app_labels.csv')\n",
    "label_categories = pd.read_csv('label_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline(train_data, test_data,events,phone_data,app_events,app_labels,label_categories):\n",
    "    start=datetime.now()\n",
    "    \n",
    "    #Data Preparation Pipeline\n",
    "    print(\"Preparing Data......\")\n",
    "    class_encoder=LabelEncoder()\n",
    "    encoded_y=class_encoder.fit_transform(train_data['group'])\n",
    "    train_data['Class']=encoded_y\n",
    "    train_data=train_data.drop(['age','gender','group'],axis=1)\n",
    "    train_devices_have_events=np.in1d(train_data['device_id'].values,events['device_id'].values)\n",
    "    train_data['has_events']=train_devices_have_events\n",
    "    test_devices_have_events=np.in1d(test_data['device_id'].values,events['device_id'].values)\n",
    "    test_data['has_events']=test_devices_have_events\n",
    "    \n",
    "    events_train_data=train_data.loc[train_data['has_events']==True]\n",
    "    events_test_data=test_data.loc[test_data['has_events']==True]\n",
    "    noevents_train_data=train_data.loc[train_data['has_events']==False]\n",
    "    noevents_test_data=test_data.loc[test_data['has_events']==False]\n",
    "    events_train_data=events_train_data.drop(['has_events'],axis=1)\n",
    "    events_test_data=events_test_data.drop(['has_events'],axis=1)\n",
    "    \n",
    "    train_data=train_data.set_index('device_id')\n",
    "    test_data=test_data.set_index('device_id')\n",
    "    events_train_data=events_train_data.set_index('device_id')\n",
    "    events_test_data=events_test_data.set_index('device_id')\n",
    "    noevents_train_data=noevents_train_data.set_index('device_id')\n",
    "    noevents_test_data=noevents_test_data.set_index('device_id')\n",
    "    \n",
    "    train_data['trainrow']=np.arange(train_data.shape[0])\n",
    "    events_train_data['trainrow']=np.arange(events_train_data.shape[0])\n",
    "    test_data['testrow']=np.arange(test_data.shape[0])\n",
    "    events_test_data['testrow']=np.arange(events_test_data.shape[0])\n",
    "    \n",
    "    noevents_train_data['trainrow']=np.arange(noevents_train_data.shape[0])\n",
    "    noevents_test_data['testrow']=np.arange(noevents_test_data.shape[0])\n",
    "    \n",
    "    brand_encoder = LabelEncoder().fit(phone_data['phone_brand'])\n",
    "    phone_data['brand'] = brand_encoder.transform(phone_data['phone_brand'])\n",
    "    nbrands=len(brand_encoder.classes_)\n",
    "\n",
    "    concat_model = phone_data['phone_brand'].str.cat(phone_data['device_model'])\n",
    "    model_encoder=LabelEncoder().fit(concat_model)\n",
    "    phone_data['model_brand']=model_encoder.transform(concat_model)\n",
    "    nmodels=len(model_encoder.classes_)\n",
    "    \n",
    "    model_encode=LabelEncoder().fit(phone_data['device_model'])\n",
    "    phone_data['model']=model_encode.transform(phone_data['device_model'])\n",
    "    num_models=len(model_encoder.classes_)\n",
    "    \n",
    "    train_data['phone_brand']=phone_data['brand']\n",
    "    test_data['phone_brand']=phone_data['brand']\n",
    "    train_data['phone_model']=phone_data['model']\n",
    "    test_data['phone_model']=phone_data['model']\n",
    "    events_train_data['phone_brand']=phone_data['brand']\n",
    "    events_test_data['phone_brand']=phone_data['brand']\n",
    "    events_train_data['phone_model']=phone_data['model_brand']\n",
    "    events_test_data['phone_model']=phone_data['model_brand']\n",
    "    \n",
    "    noevents_train_data['phone_brand']=phone_data['brand']\n",
    "    noevents_test_data['phone_brand']=phone_data['brand']\n",
    "    noevents_train_data['phone_model']=phone_data['model']\n",
    "    noevents_test_data['phone_model']=phone_data['model']\n",
    "    \n",
    "    app_encoder = LabelEncoder().fit(app_events['app_id'])\n",
    "    app_events['app'] = app_encoder.transform(app_events['app_id'])\n",
    "    napps = len(app_encoder.classes_)\n",
    "    deviceapps = (app_events.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                       .groupby(['device_id','app'])['app'].agg(['size'])\n",
    "                       .merge(events_train_data[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                       .merge(events_test_data[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                       .reset_index())\n",
    "    \n",
    "    app_labels = app_labels.loc[app_labels['app_id'].isin(app_events['app_id'].unique())]\n",
    "    app_labels['app'] = app_encoder.transform(app_labels['app_id'])\n",
    "    labelencoder = LabelEncoder().fit(app_labels['label_id'])\n",
    "    app_labels['label'] = labelencoder.transform(app_labels['label_id'])\n",
    "    nlabels = len(labelencoder.classes_)\n",
    "    \n",
    "    devicelabels = (deviceapps[['device_id','app']]\n",
    "                .merge(app_labels[['app','label']])\n",
    "                .groupby(['device_id','label'])['app'].agg(['size'])\n",
    "                .merge(events_train_data[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "                .merge(events_test_data[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                .reset_index())\n",
    "    \n",
    "    events['hour'] = events['timestamp'].map(lambda x:pd.to_datetime(x).hour)\n",
    "    events['hourbin'] = [1 if ((x>=1)&(x<=6)) else 2 if ((x>=7)&(x<=12)) else 3 if ((x>=13)&(x<=18)) else 4 for x in events['hour']]\n",
    "    hourevents = events.groupby(\"device_id\")[\"hour\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "    hourbinevents = events.groupby(\"device_id\")[\"hourbin\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "    events_train_data['event_hours']=events_train_data.index.map(hourevents)\n",
    "    events_test_data['event_hours']=events_test_data.index.map(hourevents)\n",
    "    events_train_data['event_hours_bins']=events_train_data.index.map(hourbinevents)\n",
    "    events_test_data['event_hours_bins']=events_test_data.index.map(hourbinevents)\n",
    "    \n",
    "    days_of_week=events['timestamp'].dt.day_name()\n",
    "    events['day']=days_of_week.map({'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6})\n",
    "    daysevents = events.groupby(\"device_id\")[\"day\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "    events_train_data['event_day']=events_train_data.index.map(daysevents)\n",
    "    events_test_data['event_day']=events_test_data.index.map(daysevents)\n",
    "    \n",
    "    lat_events = events.groupby(\"device_id\")[\"latitude\"].apply(lambda x: np.median([float(s) for s in x]))\n",
    "    long_events = events.groupby(\"device_id\")[\"longitude\"].apply(lambda x: np.median([float(s) for s in x]))\n",
    "    events_train_data['event_med_lat']=events_train_data.index.map(lat_events)\n",
    "    events_test_data['event_med_lat']=events_test_data.index.map(lat_events)\n",
    "    events_train_data['event_med_long']=events_train_data.index.map(long_events)\n",
    "    events_test_data['event_med_long']=events_test_data.index.map(long_events)\n",
    "    \n",
    "    appsactive = app_events.groupby(\"event_id\")[\"is_active\"].apply(lambda x: \" \".join(str(s) for s in x))\n",
    "    events[\"apps_active\"] = events.index.map(appsactive)\n",
    "    events_apps_active_map = events.groupby(\"device_id\")[\"apps_active\"].apply(lambda x: \" \".join(str(s) for s in x if str(s)!='nan'))\n",
    "    events_train_data['apps_active']=events_train_data.index.map(events_apps_active_map)\n",
    "    events_test_data['apps_active']=events_test_data.index.map(events_apps_active_map)\n",
    "    print(\"Data Preparation Complete Time Taken: \",datetime.now()-start)\n",
    "    \n",
    "    print(\"Preparing Features......\")\n",
    "    #Feature Engineering Pipeline\n",
    "    #Considering Devices with No Events Data\n",
    "    Xtr_noevents_brand = csr_matrix((np.ones(noevents_train_data.shape[0]), \n",
    "                       (noevents_train_data.trainrow, noevents_train_data.phone_brand)))\n",
    "    Xte_noevents_brand = csr_matrix((np.ones(noevents_test_data.shape[0]), \n",
    "                       (noevents_test_data.testrow, noevents_test_data.phone_brand)))\n",
    "    \n",
    "    Xtr_noevents_model = csr_matrix((np.ones(noevents_train_data.shape[0]), \n",
    "                       (noevents_train_data.trainrow, noevents_train_data.phone_model)))\n",
    "    Xte_noevents_model = csr_matrix((np.ones(noevents_test_data.shape[0]), \n",
    "                       (noevents_test_data.testrow, noevents_test_data.phone_model)))\n",
    "    X_train_noevents_one_hot=hstack((Xtr_noevents_brand,Xtr_noevents_model),format='csr')\n",
    "    X_test_noevents_one_hot=hstack((Xte_noevents_brand,Xte_noevents_model),format='csr')\n",
    "    \n",
    "    #Events Data Feature Matrix Creation\n",
    "    Xtr_events_brand = csr_matrix((np.ones(events_train_data.shape[0]), \n",
    "                       (events_train_data.trainrow, events_train_data.phone_brand)), \n",
    "                              shape=(events_train_data.shape[0],nbrands))\n",
    "    Xte_events_brand = csr_matrix((np.ones(events_test_data.shape[0]), \n",
    "                       (events_test_data.testrow, events_test_data.phone_brand)),\n",
    "                             shape=(events_test_data.shape[0],nbrands))\n",
    "    Xtr_events_model = csr_matrix((np.ones(events_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_train_data.trainrow, events_train_data.phone_model)),\n",
    "                         shape=(events_train_data.shape[0],nmodels))\n",
    "    Xte_events_model = csr_matrix((np.ones(events_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_test_data.testrow, events_test_data.phone_model)),\n",
    "                           shape=(events_test_data.shape[0],nmodels))\n",
    "    \n",
    "    d = deviceapps.dropna(subset=['trainrow'])\n",
    "    Xtr_events_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.app)), \n",
    "                      shape=(events_train_data.shape[0],napps))\n",
    "    d = deviceapps.dropna(subset=['testrow'])\n",
    "    Xte_events_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)), \n",
    "                      shape=(events_test_data.shape[0],napps))\n",
    "    d = devicelabels.dropna(subset=['trainrow'])\n",
    "    Xtr_events_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \n",
    "                      shape=(events_train_data.shape[0],nlabels))\n",
    "    d = devicelabels.dropna(subset=['testrow'])\n",
    "    Xte_events_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \n",
    "                      shape=(events_test_data.shape[0],nlabels))\n",
    "    \n",
    "    vectorizer_4=TfidfVectorizer()\n",
    "    vectorizer_4.fit(events_train_data['event_hours'].values)\n",
    "    X_tr_event_hours_one_hot = vectorizer_4.transform(events_train_data['event_hours'].values)\n",
    "    X_te_event_hours_one_hot = vectorizer_4.transform(events_test_data['event_hours'].values)\n",
    "    \n",
    "    vectorizer_5=CountVectorizer(binary=True)\n",
    "    vectorizer_5.fit(events_train_data['event_hours_bins'].values)\n",
    "    X_tr_event_hours_bins_one_hot = vectorizer_5.transform(events_train_data['event_hours_bins'].values)\n",
    "    X_te_event_hours_bins_one_hot = vectorizer_5.transform(events_test_data['event_hours_bins'].values)\n",
    "    \n",
    "    vectorizer_6=TfidfVectorizer()\n",
    "    vectorizer_6.fit(events_train_data['event_day'].values)\n",
    "    X_tr_event_day_one_hot = vectorizer_6.transform(events_train_data['event_day'].values)\n",
    "    X_te_event_day_one_hot = vectorizer_6.transform(events_test_data['event_day'].values)\n",
    "    \n",
    "    scaler_1=StandardScaler()\n",
    "    scaler_1.fit(events_train_data['event_med_lat'].values.reshape(-1,1))\n",
    "    X_tr_event_med_lat_scaled = scaler_1.transform(events_train_data['event_med_lat'].values.reshape(-1,1))\n",
    "    X_te_event_med_lat_scaled = scaler_1.transform(events_test_data['event_med_lat'].values.reshape(-1,1))\n",
    "    \n",
    "    scaler_2=StandardScaler()\n",
    "    scaler_2.fit(events_train_data['event_med_long'].values.reshape(-1,1))\n",
    "    X_tr_event_med_long_scaled = scaler_2.transform(events_train_data['event_med_long'].values.reshape(-1,1))\n",
    "    X_te_event_med_long_scaled = scaler_2.transform(events_test_data['event_med_long'].values.reshape(-1,1))\n",
    "    \n",
    "    vectorizer_8=TfidfVectorizer()\n",
    "    vectorizer_8.fit(events_train_data['apps_active'].values)\n",
    "    X_tr_apps_active_one_hot = vectorizer_8.transform(events_train_data['apps_active'].values)\n",
    "    X_te_apps_active_one_hot = vectorizer_8.transform(events_test_data['apps_active'].values)\n",
    "    \n",
    "    X_tr_event_hours_one_hot=X_tr_event_hours_one_hot.tocsr()\n",
    "    X_te_event_hours_one_hot=X_te_event_hours_one_hot.tocsr()\n",
    "    X_tr_event_hours_bins_one_hot=X_tr_event_hours_bins_one_hot.tocsr()\n",
    "    X_te_event_hours_bins_one_hot=X_te_event_hours_bins_one_hot.tocsr()\n",
    "    X_tr_event_day_one_hot=X_tr_event_day_one_hot.tocsr()\n",
    "    X_te_event_day_one_hot=X_te_event_day_one_hot.tocsr()\n",
    "    X_tr_apps_active_one_hot=X_tr_apps_active_one_hot.tocsr()\n",
    "    X_te_apps_active_one_hot=X_te_apps_active_one_hot.tocsr()\n",
    "    \n",
    "    X_train_events_one_hot_1=hstack((Xtr_events_brand,Xtr_events_model,Xtr_events_label,X_tr_event_hours_one_hot,X_tr_event_hours_bins_one_hot,X_tr_event_day_one_hot,X_tr_event_med_lat_scaled,X_tr_event_med_long_scaled,Xtr_events_app,X_tr_apps_active_one_hot),format='csr')\n",
    "    X_test_events_one_hot_1=hstack((Xte_events_brand,Xte_events_model,Xte_events_label,X_te_event_hours_one_hot,X_te_event_hours_bins_one_hot,X_te_event_day_one_hot,X_te_event_med_lat_scaled,X_te_event_med_long_scaled,Xte_events_app,X_te_apps_active_one_hot),format='csr')\n",
    "    \n",
    "    print(\"Feature Preparation Done Time Taken: \",datetime.now()-start)\n",
    "    \n",
    "    print(\"Predicting Output......\")\n",
    "    y_data=noevents_train_data['Class'].values\n",
    "    train_1, cv_1, y_train_1, y_cv_1 = train_test_split(X_train_noevents_one_hot, y_data,stratify=y_data,test_size=0.15,random_state=18)\n",
    "    test_1=X_test_noevents_one_hot\n",
    "    \n",
    "    #Loading Saved Logistic Regression Model\n",
    "    lr_model=joblib.load('Saved_Models/no_events_calibrated_logistic_regression.sav')\n",
    "    lr_no_events_train_prediction=lr_model.predict_proba(train_1)\n",
    "    lr_no_events_cv_prediction=lr_model.predict_proba(cv_1)\n",
    "    lr_no_events_test_prediction=lr_model.predict_proba(test_1)\n",
    "    \n",
    "    #Loading All 5 Saved Model_1_1 Neural Network  Models\n",
    "    model_list_1=[]\n",
    "    for i in range(5):\n",
    "        model=model_1_1(train_1.shape[1])\n",
    "        model.load_weights('Saved_Models/No_Events/Neural_Network_1/Model_1_1_'+str(i+1)+'.h5')\n",
    "        model_list_1.append(model)\n",
    "        \n",
    "    train_pred_avg_1_1=np.zeros((train_1.shape[0],12))\n",
    "    for i in range(len(model_list_1)):\n",
    "        train_pred=model_list_1[i].predict_proba(train_1)\n",
    "        train_pred_avg_1_1+=train_pred\n",
    "    train_pred_avg_1_1/=len(model_list_1)\n",
    "    \n",
    "    cv_pred_avg_1_1=np.zeros((cv_1.shape[0],12))\n",
    "    for i in range(len(model_list_1)):\n",
    "        cv_pred=model_list_1[i].predict_proba(cv_1)\n",
    "        cv_pred_avg_1_1+=cv_pred\n",
    "    cv_pred_avg_1_1/=len(model_list_1)\n",
    "    \n",
    "    test_pred_avg_1_1=np.zeros((test_1.shape[0],12))\n",
    "    for i in range(len(model_list_1)):\n",
    "        test_pred=model_list_1[i].predict_proba(test_1)\n",
    "        test_pred_avg_1_1+=test_pred\n",
    "    test_pred_avg_1_1/=len(model_list_1)\n",
    "    \n",
    "    #Loading Saved Model_1_2 Neural Network Model\n",
    "    model_1_2=create_model_1_2(train_1.shape[1],12)\n",
    "    model_1_2.load_weights('Saved_Models/No_Events/Model_1_2.h5')\n",
    "    train_pred_1_2=model_1_2.predict_proba(train_1)\n",
    "    cv_pred_1_2=model_1_2.predict_proba(cv_1)\n",
    "    test_pred_1_2=model_1_2.predict_proba(test_1)\n",
    "    \n",
    "    y_data_events=events_train_data['Class'].values\n",
    "    train_2, cv_2, y_train_2, y_cv_2 = train_test_split(X_train_events_one_hot_1, y_data_events,stratify=y_data_events,test_size=0.2,random_state=9)\n",
    "    test_2=X_test_events_one_hot_1\n",
    "    \n",
    "    #Loading All 20 Saved Model_2_1 Neural Network Models\n",
    "    model_list_2=[]\n",
    "    for i in range(20):\n",
    "        model=model_2_1(train_2.shape[1],12)\n",
    "        model.load_weights('Saved_Models/Events/Neural_Network_1/Model_2_1_'+str(i+1)+'.h5')\n",
    "        model_list_2.append(model)\n",
    "    \n",
    "    train_pred_avg_2_1=np.zeros((train_2.shape[0],12))\n",
    "    for i in range(len(model_list_2)):\n",
    "        train_pred=model_list_2[i].predict_proba(train_2)\n",
    "        train_pred_avg_2_1+=train_pred\n",
    "    train_pred_avg_2_1/=len(model_list_2)\n",
    "    \n",
    "    cv_pred_avg_2_1=np.zeros((cv_2.shape[0],12))\n",
    "    for i in range(len(model_list_2)):\n",
    "        cv_pred=model_list_2[i].predict_proba(cv_2)\n",
    "        cv_pred_avg_2_1+=cv_pred\n",
    "    cv_pred_avg_2_1/=len(model_list_2)\n",
    "    \n",
    "    test_pred_avg_2_1=np.zeros((test_2.shape[0],12))\n",
    "    for i in range(len(model_list_2)):\n",
    "        test_pred=model_list_2[i].predict_proba(test_2)\n",
    "        test_pred_avg_2_1+=test_pred\n",
    "    test_pred_avg_2_1/=len(model_list_2)\n",
    "    \n",
    "    #Loading All 20 Saved Model_2_2 Neural Network Models\n",
    "    model_list_3=[]\n",
    "    for i in range(20):\n",
    "        model=model_2_2(train_2.shape[1],12)\n",
    "        model.load_weights('Saved_Models/Events/Neural_Network_2/Model_2_2_'+str(i+1)+'.h5')\n",
    "        model_list_3.append(model)\n",
    "        \n",
    "    train_pred_avg_2_2=np.zeros((train_2.shape[0],12))\n",
    "    for i in range(len(model_list_3)):\n",
    "        train_pred=model_list_3[i].predict_proba(train_2)\n",
    "        train_pred_avg_2_2+=train_pred\n",
    "    train_pred_avg_2_2/=len(model_list_3)\n",
    "    \n",
    "    cv_pred_avg_2_2=np.zeros((cv_2.shape[0],12))\n",
    "    for i in range(len(model_list_3)):\n",
    "        cv_pred=model_list_3[i].predict_proba(cv_2)\n",
    "        cv_pred_avg_2_2+=cv_pred\n",
    "    cv_pred_avg_2_2/=len(model_list_3)\n",
    "    \n",
    "    test_pred_avg_2_2=np.zeros((test_2.shape[0],12))\n",
    "    for i in range(len(model_list_3)):\n",
    "        test_pred=model_list_3[i].predict_proba(test_2)\n",
    "        test_pred_avg_2_2+=test_pred\n",
    "    test_pred_avg_2_2/=len(model_list_3)\n",
    "    \n",
    "    print(\"Models Predictions Done Time Taken:\",datetime.now()-start)\n",
    "    \n",
    "    print(\"Ensembling Models......\")\n",
    "    w1_1=0.15\n",
    "    w1_2=0.75\n",
    "    w1_3=0.1\n",
    "    \n",
    "    w2_1=0.5\n",
    "    w2_2=0.5\n",
    "    \n",
    "    #Esembling and Calculating weighted average predictions\n",
    "    train_prediction_1=(w1_1*lr_no_events_train_prediction)+(w1_2*train_pred_avg_1_1)+(w1_3*train_pred_1_2)\n",
    "    cv_prediction_1=(w1_1*lr_no_events_cv_prediction)+(w1_2*cv_pred_avg_1_1)+(w1_3*cv_pred_1_2)\n",
    "    train_prediction_2=(w2_1*train_pred_avg_2_1)+(w2_2*train_pred_avg_2_2)\n",
    "    cv_prediction_2=(w2_1*cv_pred_avg_2_1)+(w2_2*cv_pred_avg_2_2)\n",
    "    Test_Prediction_1=(w1_1*lr_no_events_test_prediction)+(w1_2*test_pred_avg_1_1)+(w1_3*test_pred_1_2)\n",
    "    Test_Prediction_2=(w2_1*test_pred_avg_2_1)+(w2_2*test_pred_avg_2_2)\n",
    "    \n",
    "    \n",
    "    print(\"No Events Train Log-Loss: \",log_loss(y_train_1, train_prediction_1))\n",
    "    print(\"Events Train Log-Loss: \",log_loss(y_train_2, train_prediction_2))\n",
    "    print(\"No Events CV Log-Loss: \",log_loss(y_cv_1, cv_prediction_1))\n",
    "    print(\"Events CV Log-Loss: \",log_loss(y_cv_2, cv_prediction_2))\n",
    "    \n",
    "    print(\"Returned Test Predictions for Submission\")\n",
    "    print(\"Total Time Taken: \",datetime.now()-start)\n",
    "    \n",
    "    return Test_Prediction_1,Test_Prediction_2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data......\n",
      "Data Preparation Complete Time Taken:  0:03:36.871455\n",
      "Preparing Features......\n",
      "Feature Preparation Done Time Taken:  0:04:04.209560\n",
      "Predicting Output......\n",
      "Models Predictions Done Time Taken: 0:12:59.428000\n",
      "Ensembling Models......\n",
      "No Events Train Log-Loss:  2.3620290803707564\n",
      "Events Train Log-Loss:  1.606074682066437\n",
      "No Events CV Log-Loss:  2.3597141116725457\n",
      "Events CV Log-Loss:  1.8937726608417007\n",
      "Returned Test Predictions for Submission\n",
      "Total Time Taken:  0:12:59.490495\n"
     ]
    }
   ],
   "source": [
    "no_events_test_predictions,events_test_predictions=ML_pipeline(train_data, test_data,events,phone_data,app_events,app_labels,label_categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
